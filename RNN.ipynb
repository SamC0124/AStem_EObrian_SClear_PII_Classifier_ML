{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Mount Drive\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import tensor\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from string import punctuation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.589095Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "raw_data_1 = pd.read_json(f\"data/train.json\")\n",
    "raw_data_2 = pd.read_csv(f\"data/PII43k.csv\", on_bad_lines='skip')\n",
    "raw_data_2['full_text'] = raw_data_2[\"Filled Template\"]\n",
    "raw_data_2[\"tokens\"] = raw_data_2[\"Tokenised Filled Template\"]\n",
    "raw_data_2[\"labels\"] = raw_data_2[\"Tokens\"]\n",
    "raw_data_2.drop(columns=['Template', 'Filled Template', 'Tokenised Filled Template', 'Tokens'])\n",
    "all_data = pd.concat([raw_data_1, raw_data_2], ignore_index=True)\n",
    "raw_data = all_data.drop(columns=['Template', 'Filled Template', 'Tokenised Filled Template', 'Tokens'])"
   ],
   "metadata": {
    "id": "SNXmUIIYXIy1",
    "ExecuteTime": {
     "end_time": "2024-03-27T03:11:53.599079Z",
     "start_time": "2024-03-27T03:11:53.595469Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ljl4oIA5bkBR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "all_data"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n7WjMAGHMbHI",
    "outputId": "bd1f79d9-2547-4a92-b2db-a218c9931a5b",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.601394Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a set of punctuation and special characters to omit from tokens.\n",
    "omitted_characters = set(punctuation)\n",
    "omitted_characters.add(\"\\n\\n\")\n",
    "omitted_characters.add(\"\\n\")\n",
    "omitted_characters.add(\"\\r\\n\")\n",
    "omitted_characters.add(\"\\r\")\n",
    "omitted_characters.add(\" \")\n",
    "omitted_characters.add(\"â€¢\")\n",
    "#print(omitted_characters)"
   ],
   "metadata": {
    "id": "nnhAfDr2XWxj",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.607765Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_tokens(data):\n",
    "    cleaned_data = data.copy()\n",
    "    for idx in cleaned_data.index:\n",
    "        updated_tokens = []\n",
    "        tokens = cleaned_data.at[idx, 'tokens']\n",
    "        for token in tokens:\n",
    "            token = token.lower().strip()\n",
    "            if token not in omitted_characters:\n",
    "                updated_tokens.append(token)\n",
    "\n",
    "        cleaned_data.at[idx, 'tokens'] = updated_tokens\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def binarize_data(data):\n",
    "    \"\"\"\n",
    "    Makes the labels of the data binary (either 0 or 1).\n",
    "    \"\"\"\n",
    "    for index, document_labels in enumerate(data['labels']):\n",
    "        data.loc[index, 'labels'] = 0\n",
    "        for label in document_labels:\n",
    "            if label != 'O':\n",
    "                data.at[index, 'labels'] = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(list(stopwords.words('english')) + [\"and\", \"2021\", \"1999\", \"a\", \"4\", \"t.\"])\n",
    "    for row, tokens in enumerate(data['tokens']):\n",
    "        for token_index, word in enumerate(tokens):\n",
    "            if word.lower() in stop_words:\n",
    "                data['tokens'][row].pop(token_index)\n",
    "\n",
    "    return data\n",
    "\n",
    "def use_word2vec(data):\n",
    "    doc_texts = data['tokens'].tolist()\n",
    "    model = Word2Vec(doc_texts, vector_size=100, window=5, min_count=5, workers=4)\n",
    "    document_mean_vectors = []\n",
    "    for doc in doc_texts:\n",
    "        #new vector of words for each document\n",
    "        word_vectors = []\n",
    "        for word in doc:\n",
    "            if word in model.wv:\n",
    "              word_vectors.append(model.wv[word])\n",
    "            else:\n",
    "              word_vectors.append(np.zeros(model.vector_size))\n",
    "        #calculate mean vector for the document\n",
    "        document_mean_vectors.append(np.mean(word_vectors, axis=0))\n",
    "\n",
    "    #this should be X when doing test/train/split\n",
    "    return document_mean_vectors"
   ],
   "metadata": {
    "id": "e-BBELzwXYrv",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.613993Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Clean data\n",
    "\n",
    "#raw_data = create_sentence_tokens(raw_data)\n",
    "cleaned_data = binarize_data(raw_data)\n",
    "cleaned_data = clean_tokens(cleaned_data)\n",
    "cleaned_data = remove_stopwords(cleaned_data)\n",
    "# cleaned_data.drop(['full_text', 'trailing_whitespace', 'document'], axis=1, inplace=True)\n",
    "cleaned_data.head()"
   ],
   "metadata": {
    "id": "evhN_TDhXle0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "outputId": "b3b9a001-ca2c-4121-dcb6-eb56997b7a20",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.617467Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Prepare X and y for the model\n",
    "y = cleaned_data['labels']\n",
    "y = y.astype(int)\n",
    "\n",
    "X = use_word2vec(cleaned_data)\n",
    "#X = cleaned_data.drop(['labels', 'document', 'tokens', 'trailing_whitespace'], axis=1)\n",
    "corpus = cleaned_data['full_text']"
   ],
   "metadata": {
    "id": "WKozD-V3XnGN",
    "ExecuteTime": {
     "end_time": "2024-03-27T03:11:53.656532Z",
     "start_time": "2024-03-27T03:11:53.620033Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ],
   "metadata": {
    "id": "iRkfyFyeF6LS",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.623550Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "voc_size=50000 #how many unique words do I have"
   ],
   "metadata": {
    "id": "_qH36RIgF7eF",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.626348Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "onehot_repr=[one_hot(words,voc_size)for words in corpus]\n",
    "\n",
    "sent_length=400 #how many words in a sentence\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='post',maxlen=sent_length)"
   ],
   "metadata": {
    "id": "0n5N9wC0Fk-L",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.629705Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_vector_features= 100 ##features representation - every index will be represented by this many features\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ],
   "metadata": {
    "id": "agXWOokoFpzH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7813af4e-7130-4f2b-9a34-3308835f03d1",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.632129Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "X_final=np.array(embedded_docs)\n",
    "y_final=y"
   ],
   "metadata": {
    "id": "tIwdj59hGDrJ",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.634683Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_final"
   ],
   "metadata": {
    "id": "lD6EXL5WOZhZ",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.637551Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_final"
   ],
   "metadata": {
    "id": "gS2xmcgiObHy",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.641746Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_final.shape,y_final.shape"
   ],
   "metadata": {
    "id": "ZbcHiIh0GFIu",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.645564Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=5)"
   ],
   "metadata": {
    "id": "TKW5m-FvcirW",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.649641Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "tav-_K0HWttx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=3,batch_size=128)"
   ],
   "metadata": {
    "id": "HPihtBdTW8oT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9192c59c-fe3f-40bb-fc0d-b1e61716a5e9",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.653731Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_pred=model.predict(X_test)"
   ],
   "metadata": {
    "id": "PKRo-e1PE3qA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aaa90f8a-12a0-4afb-8d19-5967d8a019de",
    "ExecuteTime": {
     "end_time": "2024-03-27T03:11:53.711843Z",
     "start_time": "2024-03-27T03:11:53.657189Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_pred=np.where(y_pred > 0.5, 1,0)"
   ],
   "metadata": {
    "id": "CE-8rRofGRTE",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.659719Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ],
   "metadata": {
    "id": "t2KQkqjbGVWS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d28f092d-4557-4736-e3a5-e83ea0b62261",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.663033Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)\n",
    "\n",
    "# This code was references from W3Schools: https://w3schools.com/python/python_ml_auc_roc.asp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "def plot_roc_curve(true_y, y_prob):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(y_test, y_pred)"
   ],
   "metadata": {
    "id": "OAH_ASFYGYaF",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "outputId": "0b9f5759-8bed-4cf0-d5b4-54fb993c32af",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.666287Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "id": "sCQUcUEJGaEn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "41a9b84e-d01b-41a4-b8b1-1093849a4a70",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.669267Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "53yVAPIub35t",
    "ExecuteTime": {
     "start_time": "2024-03-27T03:11:53.672184Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
